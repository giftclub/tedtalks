{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# title: <span class=\"player-hero__title__content\">\n",
    "# description: <p class=\"talk-description\" lang=\"en\">\n",
    "# views: <span class=\"talk-sharing__value\"> 96,013 </span>\n",
    "\n",
    "\n",
    "def parse(soup):\n",
    "    # both title and views are can be parsed in separate tags.\n",
    "    title = soup.find('span', {'class' : \"player-hero__title__content\"}).text.strip('\\n')\n",
    "    views = soup.find('span', {'class' : \"talk-sharing__value\"}).text.strip('\\n')\n",
    "    descr = soup.find('p', {'class' : \"talk-description\"}).text.strip('\\n')\n",
    "    return title.strip(), views, descr\n",
    "\n",
    "def to_csv(pth, out):\n",
    "    # open file to write to.\n",
    "    with open(out, \"w\") as out:\n",
    "        # create csv.writer.\n",
    "        wr = csv.writer(out)\n",
    "        # write our headers.\n",
    "        wr.writerow([\"title\", \"views\", \"descr\"])\n",
    "        # get all our html files.\n",
    "        for html in os.listdir(pth):\n",
    "            with open(os.path.join(pth, html)) as f:\n",
    "                print(html)\n",
    "                # parse the file and write the data to a row.\n",
    "                wr.writerow(parse(BeautifulSoup(f, \"lxml\")))\n",
    "\n",
    "to_csv(\"./html_files/descriptions/\",\"descriptions-2.csv\") # This is to the test directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "colnames = ['title', 'views' , 'descr']\n",
    "data = pandas.read_csv('./data/descriptions-2.csv', names=colnames)\n",
    "titles = data.title.tolist()\n",
    "views = data.views.tolist()\n",
    "descriptions = data.descr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(descriptions[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split()\n",
    " \n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(descriptions)\n",
    "#print(sklearn_representation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# To get a list of punctuation:\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-96-7d76c83bc369>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-96-7d76c83bc369>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(ord(somewords(16))\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# To find out the ordinal of a particular vexing character:\n",
    "\n",
    "somewords = descriptions[2]\n",
    "print(ord(somewords(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021739130434782608"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "def ourtokens(ourstring):\n",
    "    \n",
    "    stoplist = set(get_stop_words('en'))\n",
    "    finalList = []\n",
    "    \n",
    "    wordList = ourstring.lower().split()\n",
    "    for i in range(len(wordList)):\n",
    "        #wordList[i] = re.sub('[^a-zA-Z\\']', '', wordList[i]).strip(chr(8212)) \n",
    "        #NOTE: the above left spaces and added empty strings\n",
    "        \n",
    "        no_punc = wordList[i].strip(string.punctuation) #remove most punctuation\n",
    "        no_emphwhatever = no_punc.strip(chr(8212)) # remove that weirdness\n",
    "        no_num = no_emphwhatever.strip(string.digits) #remove numbers\n",
    "        \n",
    "        if (len(no_num) > 0) and (no_num not in stoplist): # Requires stop_words\n",
    "            # First conditional stops empty strings from being added\n",
    "            finalList.append(no_num)\n",
    "            \n",
    "    return finalList\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    #print(intersection)\n",
    "    union = set(query).union(set(document))\n",
    "    #print(union)\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "jaccard_similarity(ourtokens(descriptions[1]), ourtokens(descriptions[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows completed\n",
      "100 rows completed\n",
      "200 rows completed\n",
      "300 rows completed\n",
      "400 rows completed\n",
      "500 rows completed\n",
      "600 rows completed\n",
      "700 rows completed\n",
      "800 rows completed\n",
      "900 rows completed\n",
      "1000 rows completed\n",
      "1100 rows completed\n",
      "1200 rows completed\n",
      "1300 rows completed\n",
      "1400 rows completed\n",
      "1500 rows completed\n",
      "1600 rows completed\n",
      "1700 rows completed\n",
      "1800 rows completed\n",
      "1900 rows completed\n",
      "2000 rows completed\n",
      "2100 rows completed\n",
      "2200 rows completed\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "des_word_lists = []\n",
    "for i in range(len(descriptions)):\n",
    "    # Create list of words for each description\n",
    "    words = ourtokens(descriptions[i])\n",
    "    des_word_lists.append({'descriptions': descriptions[i], 'words': words})\n",
    "    \n",
    "    # Tells you where you are in the rows\n",
    "    if (i % 100) == 0:\n",
    "        print(str(i) + \" rows completed\")\n",
    "                           \n",
    "# Creates the new CSV\n",
    "with open('desPlusWords.csv', 'w') as desfile:\n",
    "    fields = ['descriptions', 'words']\n",
    "    writer = csv.DictWriter(desfile, fieldnames = fields)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    writer.writerows(des_word_lists)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows completed\n",
      "100 rows completed\n",
      "200 rows completed\n",
      "300 rows completed\n",
      "400 rows completed\n",
      "500 rows completed\n",
      "600 rows completed\n",
      "700 rows completed\n",
      "800 rows completed\n",
      "900 rows completed\n",
      "1000 rows completed\n",
      "1100 rows completed\n",
      "1200 rows completed\n",
      "1300 rows completed\n",
      "1400 rows completed\n",
      "1500 rows completed\n",
      "1600 rows completed\n",
      "1700 rows completed\n",
      "1800 rows completed\n",
      "1900 rows completed\n",
      "2000 rows completed\n",
      "2100 rows completed\n",
      "2200 rows completed\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "Ndes = len(des_word_lists)\n",
    "\n",
    "# From http://stackoverflow.com/questions/568962/how-do-i-create-an-empty-array-matrix-in-numpy\n",
    "jac_mat = numpy.zeros(shape=(Ndes,Ndes))\n",
    "\n",
    "for i in range(Ndes):\n",
    "    if (i % 100) == 0:\n",
    "        print(str(i) + \" rows completed\")  \n",
    "    \n",
    "    # Start the pairwise computations\n",
    "    \n",
    "    for j in range((i+1),Ndes):\n",
    "        # Pull the ith and jth document\n",
    "        doc_i = des_word_lists[i]['words']\n",
    "        doc_j = des_word_lists[j]['words']\n",
    "        \n",
    "        # Get the Jaccard similarity\n",
    "        jac_ij = jaccard_similarity(doc_i, doc_j)\n",
    "        \n",
    "        # Since the Jaccard will be the same between i and j as it will between\n",
    "        # j and i, we set JAC_MAT[i,j] and JAC_MAT[j,i] to be the same value\n",
    "        jac_mat[i,j] = jac_ij\n",
    "        jac_mat[j,i] = jac_ij\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.304347826087\n"
     ]
    }
   ],
   "source": [
    "# This block find the maximum for the matrix\n",
    "\n",
    "# Initialize the max to be zero. \n",
    "mat_max = 0\n",
    "\n",
    "# Loop over all the rows\n",
    "for i in range(Ndes):\n",
    "    # Find the maximum for each row\n",
    "    row_max = max(jac_mat[i])\n",
    "    \n",
    "    # Check if the current row's maximum is higher than the current MAT_MAX.\n",
    "    # If the row maximum is bigger, then set MAT_MAX to the row maximum.\n",
    "    if row_max > mat_max:\n",
    "        mat_max = row_max\n",
    "\n",
    "print(mat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0097816531347 0.304347826087\n"
     ]
    }
   ],
   "source": [
    "jac_mean = jac_mat.mean()\n",
    "print(jac_mean, mat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean is 0.009781653134697772, and the max is 0.30434782608695654.\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean is {}, and the max is {}.\".format(jac_mean, mat_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.from_numpy_matrix(jac_mat)\n",
    "nx.draw(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
